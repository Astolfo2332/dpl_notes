{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from modular_classification import (data_setup, \n",
    "                                    engine)\n",
    "from modular_classification.engine import test_func                                 \n",
    "import torchinfo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int=23):\n",
    "    \"\"\"\n",
    "    Sets a specific seed for the tests\n",
    "    \n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults is 23.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir= \"./datasets/pizza_sushi/test\"\n",
    "train_dir = \"./datasets/pizza_sushi/train\"\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "auto_transforms = weights.transforms()\n",
    "train_dataloader, test_dataloader,class_names = data_setup.create_dataloaders(train_dir=train_dir\n",
    "                                                                               , test_dir=test_dir\n",
    "                                                                                ,transform=auto_transforms,\n",
    "                                                                                  batch_size=32)\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)                                                                          \n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "torch.manual_seed(23)\n",
    "torch.cuda.manual_seed(23)\n",
    "\n",
    "output_shape = len(class_names)\n",
    "\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), #We dont change this variable\n",
    "    torch.nn.Linear(in_features=1280, out_features=output_shape, bias=True)\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 450\n",
       "     Root location: ./datasets/pizza_sushi/train\n",
       "     StandardTransform\n",
       " Transform: ImageClassification(\n",
       "                crop_size=[224]\n",
       "                resize_size=[256]\n",
       "                mean=[0.485, 0.456, 0.406]\n",
       "                std=[0.229, 0.224, 0.225]\n",
       "                interpolation=InterpolationMode.BICUBIC\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 150\n",
       "     Root location: ./datasets/pizza_sushi/test\n",
       "     StandardTransform\n",
       " Transform: ImageClassification(\n",
       "                crop_size=[224]\n",
       "                resize_size=[256]\n",
       "                mean=[0.485, 0.456, 0.406]\n",
       "                std=[0.229, 0.224, 0.225]\n",
       "                interpolation=InterpolationMode.BICUBIC\n",
       "            ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset, test_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type (var_name))                                      Param #                   Trainable\n",
       "==============================================================================================================\n",
       "EfficientNet (EfficientNet)                                  --                        Partial\n",
       "├─Sequential (features)                                      --                        False\n",
       "│    └─Conv2dNormActivation (0)                              --                        False\n",
       "│    │    └─Conv2d (0)                                       (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  (64)                      False\n",
       "│    │    └─SiLU (2)                                         --                        --\n",
       "│    └─Sequential (1)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (1,448)                   False\n",
       "│    └─Sequential (2)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       (10,710)                  False\n",
       "│    └─Sequential (3)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (15,350)                  False\n",
       "│    │    └─MBConv (1)                                       (31,290)                  False\n",
       "│    └─Sequential (4)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (37,130)                  False\n",
       "│    │    └─MBConv (1)                                       (102,900)                 False\n",
       "│    │    └─MBConv (2)                                       (102,900)                 False\n",
       "│    └─Sequential (5)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (126,004)                 False\n",
       "│    │    └─MBConv (1)                                       (208,572)                 False\n",
       "│    │    └─MBConv (2)                                       (208,572)                 False\n",
       "│    └─Sequential (6)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (262,492)                 False\n",
       "│    │    └─MBConv (1)                                       (587,952)                 False\n",
       "│    │    └─MBConv (2)                                       (587,952)                 False\n",
       "│    │    └─MBConv (3)                                       (587,952)                 False\n",
       "│    └─Sequential (7)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (717,232)                 False\n",
       "│    └─Conv2dNormActivation (8)                              --                        False\n",
       "│    │    └─Conv2d (0)                                       (409,600)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  (2,560)                   False\n",
       "│    │    └─SiLU (2)                                         --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                --                        --\n",
       "├─Sequential (classifier)                                    --                        True\n",
       "│    └─Dropout (0)                                           --                        --\n",
       "│    └─Linear (1)                                            3,843                     True\n",
       "==============================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "==============================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model,(32, 3, 224, 224), col_names=[\"num_params\",\"trainable\"]\n",
    "                  ,row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify our existing train and test function to integrate the write funtion of the tensorboard writer, its usage is **writer.add_scalars(main_tag, tag_scalar_dict)** \\\n",
    "main_tag = the metric\n",
    "tag scalar = a dict with the results like {\"train_loss\": int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, test_data: DataLoader, train_data: DataLoader, loss_fn:nn.Module,\n",
    "        optimizer: torch.optim.Optimizer, device: torch.device, epochs: int) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Trains and test a Pytorch model\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        model: A Pytorch model\n",
    "        train_data: a DataLoader object with the train data\n",
    "        test_data: a DataLoader object with the train data\n",
    "        loss_fn: loss function to minimized\n",
    "        optimizer: A Pytorch optimizer\n",
    "        device: A target device to perform the operations (\"cuda\" or \"cpu\")\n",
    "        epochs: A integre with the number of epochs that the model will be train\n",
    "    Returns:\n",
    "    --------\n",
    "        A dictionary with the train and test loss and accuracy for every epoch\n",
    "        in the form of \n",
    "        {\"train_loss\": [...],\n",
    "        \"train_acc\": [...],\n",
    "        \"test_loss\": [...],\n",
    "        \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_func(\n",
    "            model,train_data,loss_fn,optimizer, device)\n",
    "        test_loss, test_acc = test_func(\n",
    "            model, test_data, loss_fn, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} |\"\n",
    "            f\"train_loss :{train_loss: .4f} |\"\n",
    "            f\"train_acc :{train_acc: .4f} |\"\n",
    "            f\"test_loss :{test_loss: .4f} |\"\n",
    "            f\"test_acc :{test_acc: .4f} \"\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        ##Add of the writer\n",
    "        writer.add_scalars(main_tag=\"Loss\", \n",
    "        tag_scalar_dict={\"train_loss\": train_loss, \"tests_loss\": test_loss},\n",
    "        global_step=epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(main_tag=\"Accuracy\", \n",
    "        tag_scalar_dict={\"train_acc\": train_acc, \"tests_acc\": test_acc},\n",
    "        global_step=epoch\n",
    "        )\n",
    "\n",
    "        writer.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cfe55061b54887b737d4e50614e539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.3345 |train_acc : 0.3063 |test_loss : 0.7612 |test_acc : 0.6557 \n",
      "Epoch 2 |train_loss : 0.4806 |train_acc : 0.9333 |test_loss : 0.3874 |test_acc : 0.9534 \n",
      "Epoch 3 |train_loss : 0.3952 |train_acc : 0.9187 |test_loss : 0.2825 |test_acc : 0.9597 \n",
      "Epoch 4 |train_loss : 0.3089 |train_acc : 0.9375 |test_loss : 0.2341 |test_acc : 0.9722 \n",
      "Epoch 5 |train_loss : 0.2478 |train_acc : 0.9521 |test_loss : 0.2055 |test_acc : 0.9722 \n",
      "Epoch 6 |train_loss : 0.2096 |train_acc : 0.9583 |test_loss : 0.1836 |test_acc : 0.9722 \n",
      "Epoch 7 |train_loss : 0.1834 |train_acc : 0.9646 |test_loss : 0.1668 |test_acc : 0.9722 \n",
      "Epoch 8 |train_loss : 0.1632 |train_acc : 0.9708 |test_loss : 0.1538 |test_acc : 0.9722 \n",
      "Epoch 9 |train_loss : 0.1467 |train_acc : 0.9729 |test_loss : 0.1435 |test_acc : 0.9722 \n",
      "Epoch 10 |train_loss : 0.1328 |train_acc : 0.9812 |test_loss : 0.1351 |test_acc : 0.9722 \n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "results = train(model, test_dataloader, train_dataloader, loss_fn, optimizer, device, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets defying a function that creates a writer base in time and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_write(name: str, model: str, extra: str=None) -> SummaryWriter():\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    if extra:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, name, model, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, name, model)\n",
    "    print(f\"[INFO] create summary writer, saving to: {log_dir}\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model: nn.Module, data: DataLoader, loss_fn:nn.Module,\n",
    "               optimizer: torch.optim.Optimizer, device: torch.device\n",
    "               , log_dir: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Trains a model for a single epoch\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "        model: A Pytorch model\n",
    "        data: a DataLoader object with the train data\n",
    "        loss_fn: loss function to minimized\n",
    "        optimizer: A Pytorch optimizer\n",
    "        device: A target device to perform the operations (\"cuda\" or \"cpu\")\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "        A tuple with the loss and accuracy of the training epoch like\n",
    "        (train_loss, train_acc)\n",
    "    \"\"\"\n",
    "    def trace_handler(prof):\n",
    "        print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=1,\n",
    "            warmup=1,\n",
    "            active=1,\n",
    "            repeat=1),\n",
    "        #on_trace_ready=trace_handler,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\"),\n",
    "        with_stack=True\n",
    "    ) as profiler:\n",
    "        for _ , (x, y) in enumerate(data):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_logit = model(x)\n",
    "            loss = loss_fn(y_logit, y)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            profiler.step()\n",
    "            y_pred = torch.softmax(y_logit, 1).argmax(1)\n",
    "            train_acc += (y_pred == y).sum().item() / len (y_pred)\n",
    "            #We can use another function if we want\n",
    "    train_loss = train_loss / len(data)\n",
    "    train_acc = train_acc / len(data)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, test_data: DataLoader, train_data: DataLoader, loss_fn:nn.Module,\n",
    "        optimizer: torch.optim.Optimizer, device: torch.device, epochs: int\n",
    "        , writer: SummaryWriter, log_dir: str) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Trains and test a Pytorch model\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        model: A Pytorch model\n",
    "        train_data: a DataLoader object with the train data\n",
    "        test_data: a DataLoader object with the train data\n",
    "        loss_fn: loss function to minimized\n",
    "        optimizer: A Pytorch optimizer\n",
    "        device: A target device to perform the operations (\"cuda\" or \"cpu\")\n",
    "        epochs: A integre with the number of epochs that the model will be train\n",
    "    Returns:\n",
    "    --------\n",
    "        A dictionary with the train and test loss and accuracy for every epoch\n",
    "        in the form of \n",
    "        {\"train_loss\": [...],\n",
    "        \"train_acc\": [...],\n",
    "        \"test_loss\": [...],\n",
    "        \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_func(\n",
    "            model,train_data,loss_fn,optimizer, device, log_dir)\n",
    "        test_loss, test_acc = test_func(\n",
    "            model, test_data, loss_fn, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} |\"\n",
    "            f\"train_loss :{train_loss: .4f} |\"\n",
    "            f\"train_acc :{train_acc: .4f} |\"\n",
    "            f\"test_loss :{test_loss: .4f} |\"\n",
    "            f\"test_acc :{test_acc: .4f} \"\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        ##Add of the writer\n",
    "        if writer:\n",
    "            writer.add_scalars(main_tag=\"Loss\", \n",
    "            tag_scalar_dict={\"train_loss\": train_loss, \"tests_loss\": test_loss},\n",
    "            global_step=epoch\n",
    "            )\n",
    "\n",
    "            writer.add_scalars(main_tag=\"Accuracy\", \n",
    "            tag_scalar_dict={\"train_acc\": train_acc, \"tests_acc\": test_acc},\n",
    "            global_step=epoch\n",
    "            )\n",
    "\n",
    "            writer.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some experiments with both the models and using different optimiztors, changing the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "def create_effntb0():\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=0.2, inplace=True), #We dont change this variable\n",
    "        torch.nn.Linear(in_features=1280, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "    print(f\"[INFO] create new effntb0 model.\")\n",
    "    return model\n",
    "\n",
    "def create_effntb2():\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=0.3, inplace=True), #We dont change this variable\n",
    "        torch.nn.Linear(in_features=1408, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "    print(f\"[INFO] create new effntb2 model.\")\n",
    "    return model\n",
    "\n",
    "def create_convnext_tiny():\n",
    "    weights = torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT\n",
    "    model = torchvision.models.convnext_tiny(weights=weights).to(device)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
    "        torch.nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        torch.nn.Linear(in_features=768, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "    print(f\"[INFO] create new convnext_tiny model.\")\n",
    "    return model\n",
    "\n",
    "def create_convnext_small():\n",
    "    weights = torchvision.models.ConvNeXt_Small_Weights.DEFAULT\n",
    "    model = torchvision.models.convnext_small(weights=weights).to(device)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
    "        torch.nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        torch.nn.Linear(in_features=768, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "    print(f\"[INFO] create new convnext_small model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(model: nn.Module, optimizer: str):\n",
    "    if optimizer == \"Adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    else: \n",
    "        return torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] create new effntb0 model.\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create new convnext_tiny model.\n",
      "[INFO] create new convnext_small model.\n"
     ]
    }
   ],
   "source": [
    "effnetb0 = create_effntb0()\n",
    "effnetb2 = create_effntb2()\n",
    "convtiny = create_convnext_tiny()\n",
    "convsmall = create_convnext_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type (var_name))                                      Param #                   Trainable\n",
       "==============================================================================================================\n",
       "EfficientNet (EfficientNet)                                  --                        Partial\n",
       "├─Sequential (features)                                      --                        False\n",
       "│    └─Conv2dNormActivation (0)                              --                        False\n",
       "│    │    └─Conv2d (0)                                       (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  (64)                      False\n",
       "│    │    └─SiLU (2)                                         --                        --\n",
       "│    └─Sequential (1)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (1,448)                   False\n",
       "│    │    └─MBConv (1)                                       (612)                     False\n",
       "│    └─Sequential (2)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       (10,710)                  False\n",
       "│    │    └─MBConv (2)                                       (10,710)                  False\n",
       "│    └─Sequential (3)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (16,518)                  False\n",
       "│    │    └─MBConv (1)                                       (43,308)                  False\n",
       "│    │    └─MBConv (2)                                       (43,308)                  False\n",
       "│    └─Sequential (4)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (50,300)                  False\n",
       "│    │    └─MBConv (1)                                       (123,750)                 False\n",
       "│    │    └─MBConv (2)                                       (123,750)                 False\n",
       "│    │    └─MBConv (3)                                       (123,750)                 False\n",
       "│    └─Sequential (5)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (149,158)                 False\n",
       "│    │    └─MBConv (1)                                       (237,870)                 False\n",
       "│    │    └─MBConv (2)                                       (237,870)                 False\n",
       "│    │    └─MBConv (3)                                       (237,870)                 False\n",
       "│    └─Sequential (6)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (301,406)                 False\n",
       "│    │    └─MBConv (1)                                       (686,868)                 False\n",
       "│    │    └─MBConv (2)                                       (686,868)                 False\n",
       "│    │    └─MBConv (3)                                       (686,868)                 False\n",
       "│    │    └─MBConv (4)                                       (686,868)                 False\n",
       "│    └─Sequential (7)                                        --                        False\n",
       "│    │    └─MBConv (0)                                       (846,900)                 False\n",
       "│    │    └─MBConv (1)                                       (1,888,920)               False\n",
       "│    └─Conv2dNormActivation (8)                              --                        False\n",
       "│    │    └─Conv2d (0)                                       (495,616)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  (2,816)                   False\n",
       "│    │    └─SiLU (2)                                         --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                --                        --\n",
       "├─Sequential (classifier)                                    --                        True\n",
       "│    └─Dropout (0)                                           --                        --\n",
       "│    └─Linear (1)                                            4,227                     True\n",
       "==============================================================================================================\n",
       "Total params: 7,705,221\n",
       "Trainable params: 4,227\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (Units.GIGABYTES): 21.04\n",
       "==============================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.53\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 5067.62\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(effnetb2, (32, 3, 224, 224), col_names=[\"num_params\",\"trainable\"]\n",
    "                  ,row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the list with the information that we want to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = [2, 5, 10]\n",
    "models_names = [\"effnetb0\", \"effnetb2\"]\n",
    "optimizers = [\"Adam\", \"SGD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the same size and transformation for all the models for the sake of consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment number: 1\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: Adam\n",
      "[INFO] Epochs: 2\n",
      "[INFO] create new effntb0 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/Adam/effnetb0/2\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193ba39f85a744ce8883dd38f8981e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.7748 |train_acc : 0.2521 |test_loss : 1.3398 |test_acc : 0.3625 \n",
      "Epoch 2 |train_loss : 1.0484 |train_acc : 0.3875 |test_loss : 1.0097 |test_acc : 0.4528 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 2\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: SGD\n",
      "[INFO] Epochs: 2\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/SGD/effnetb0/2\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ba5d4fc84740c0a7fde9c0d20785b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.1177 |train_acc : 0.2938 |test_loss : 1.0983 |test_acc : 0.3898 \n",
      "Epoch 2 |train_loss : 1.1257 |train_acc : 0.3000 |test_loss : 1.0995 |test_acc : 0.3489 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 3\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: Adam\n",
      "[INFO] Epochs: 2\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/Adam/effnetb0/2\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0181f21f41a745d1970fec9a5a977fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.7821 |train_acc : 0.2625 |test_loss : 1.2120 |test_acc : 0.3438 \n",
      "Epoch 2 |train_loss : 1.0481 |train_acc : 0.4000 |test_loss : 1.0425 |test_acc : 0.4750 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 4\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: SGD\n",
      "[INFO] Epochs: 2\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/SGD/effnetb0/2\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424469c7c6be439081bd071f504f088f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.1177 |train_acc : 0.2938 |test_loss : 1.0983 |test_acc : 0.3898 \n",
      "Epoch 2 |train_loss : 1.1257 |train_acc : 0.3000 |test_loss : 1.0995 |test_acc : 0.3489 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 5\n",
      "[INFO] model: effnetb2\n",
      "[INFO] Optimizer: Adam\n",
      "[INFO] Epochs: 5\n",
      "[INFO] create new effntb0 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/Adam/effnetb2/5\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a832e8c21d495f99c3db4d4ef34521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.7748 |train_acc : 0.2521 |test_loss : 1.3398 |test_acc : 0.3625 \n",
      "Epoch 2 |train_loss : 1.0484 |train_acc : 0.3875 |test_loss : 1.0097 |test_acc : 0.4528 \n",
      "Epoch 3 |train_loss : 1.2310 |train_acc : 0.3146 |test_loss : 1.0018 |test_acc : 0.5011 \n",
      "Epoch 4 |train_loss : 1.2538 |train_acc : 0.1521 |test_loss : 1.0056 |test_acc : 0.5750 \n",
      "Epoch 5 |train_loss : 1.1793 |train_acc : 0.1938 |test_loss : 0.9924 |test_acc : 0.5750 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 6\n",
      "[INFO] model: effnetb2\n",
      "[INFO] Optimizer: SGD\n",
      "[INFO] Epochs: 5\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/SGD/effnetb2/5\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a9d4d300fd4db08411bbc05640bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.1177 |train_acc : 0.2938 |test_loss : 1.0983 |test_acc : 0.3898 \n",
      "Epoch 2 |train_loss : 1.1257 |train_acc : 0.3000 |test_loss : 1.0995 |test_acc : 0.3489 \n",
      "Epoch 3 |train_loss : 1.1109 |train_acc : 0.3250 |test_loss : 1.0966 |test_acc : 0.3483 \n",
      "Epoch 4 |train_loss : 1.1160 |train_acc : 0.3375 |test_loss : 1.0948 |test_acc : 0.3483 \n",
      "Epoch 5 |train_loss : 1.1066 |train_acc : 0.3521 |test_loss : 1.0937 |test_acc : 0.3659 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 7\n",
      "[INFO] model: effnetb2\n",
      "[INFO] Optimizer: Adam\n",
      "[INFO] Epochs: 5\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/Adam/effnetb2/5\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e97afefbc6497393ce1d32d42e41f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.7821 |train_acc : 0.2625 |test_loss : 1.2120 |test_acc : 0.3438 \n",
      "Epoch 2 |train_loss : 1.0481 |train_acc : 0.4000 |test_loss : 1.0425 |test_acc : 0.4750 \n",
      "Epoch 3 |train_loss : 1.2253 |train_acc : 0.3187 |test_loss : 1.0436 |test_acc : 0.4545 \n",
      "Epoch 4 |train_loss : 1.2577 |train_acc : 0.2125 |test_loss : 1.0442 |test_acc : 0.4909 \n",
      "Epoch 5 |train_loss : 1.1849 |train_acc : 0.2333 |test_loss : 1.0304 |test_acc : 0.5250 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 8\n",
      "[INFO] model: effnetb2\n",
      "[INFO] Optimizer: SGD\n",
      "[INFO] Epochs: 5\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/SGD/effnetb2/5\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261ac97828e1400c821949212629fc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.1177 |train_acc : 0.2938 |test_loss : 1.0983 |test_acc : 0.3898 \n",
      "Epoch 2 |train_loss : 1.1257 |train_acc : 0.3000 |test_loss : 1.0995 |test_acc : 0.3489 \n",
      "Epoch 3 |train_loss : 1.1109 |train_acc : 0.3250 |test_loss : 1.0966 |test_acc : 0.3483 \n",
      "Epoch 4 |train_loss : 1.1160 |train_acc : 0.3375 |test_loss : 1.0948 |test_acc : 0.3483 \n",
      "Epoch 5 |train_loss : 1.1066 |train_acc : 0.3521 |test_loss : 1.0937 |test_acc : 0.3659 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 9\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: Adam\n",
      "[INFO] Epochs: 10\n",
      "[INFO] create new effntb0 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/Adam/effnetb0/10\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de8e026d304400d8da2e10c253c1b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.7748 |train_acc : 0.2521 |test_loss : 1.3398 |test_acc : 0.3625 \n",
      "Epoch 2 |train_loss : 1.0484 |train_acc : 0.3875 |test_loss : 1.0097 |test_acc : 0.4528 \n",
      "Epoch 3 |train_loss : 1.2310 |train_acc : 0.3146 |test_loss : 1.0018 |test_acc : 0.5011 \n",
      "Epoch 4 |train_loss : 1.2538 |train_acc : 0.1521 |test_loss : 1.0056 |test_acc : 0.5750 \n",
      "Epoch 5 |train_loss : 1.1793 |train_acc : 0.1938 |test_loss : 0.9924 |test_acc : 0.5750 \n",
      "Epoch 6 |train_loss : 1.1775 |train_acc : 0.2479 |test_loss : 0.9811 |test_acc : 0.5875 \n",
      "Epoch 7 |train_loss : 1.1732 |train_acc : 0.2604 |test_loss : 0.9726 |test_acc : 0.5938 \n",
      "Epoch 8 |train_loss : 1.1750 |train_acc : 0.2604 |test_loss : 0.9681 |test_acc : 0.6125 \n",
      "Epoch 9 |train_loss : 1.1725 |train_acc : 0.2750 |test_loss : 0.9601 |test_acc : 0.6062 \n",
      "Epoch 10 |train_loss : 1.1721 |train_acc : 0.2833 |test_loss : 0.9569 |test_acc : 0.6278 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 10\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: SGD\n",
      "[INFO] Epochs: 10\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/SGD/effnetb0/10\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abe5fb4b0d444f581602d3ac898ea80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.1177 |train_acc : 0.2938 |test_loss : 1.0983 |test_acc : 0.3898 \n",
      "Epoch 2 |train_loss : 1.1257 |train_acc : 0.3000 |test_loss : 1.0995 |test_acc : 0.3489 \n",
      "Epoch 3 |train_loss : 1.1109 |train_acc : 0.3250 |test_loss : 1.0966 |test_acc : 0.3483 \n",
      "Epoch 4 |train_loss : 1.1160 |train_acc : 0.3375 |test_loss : 1.0948 |test_acc : 0.3483 \n",
      "Epoch 5 |train_loss : 1.1066 |train_acc : 0.3521 |test_loss : 1.0937 |test_acc : 0.3659 \n",
      "Epoch 6 |train_loss : 1.1101 |train_acc : 0.3500 |test_loss : 1.0914 |test_acc : 0.3875 \n",
      "Epoch 7 |train_loss : 1.1101 |train_acc : 0.3667 |test_loss : 1.0914 |test_acc : 0.3688 \n",
      "Epoch 8 |train_loss : 1.1038 |train_acc : 0.3771 |test_loss : 1.0886 |test_acc : 0.4051 \n",
      "Epoch 9 |train_loss : 1.1181 |train_acc : 0.3458 |test_loss : 1.0868 |test_acc : 0.4023 \n",
      "Epoch 10 |train_loss : 1.0967 |train_acc : 0.3646 |test_loss : 1.0864 |test_acc : 0.4051 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 11\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: Adam\n",
      "[INFO] Epochs: 10\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/Adam/effnetb0/10\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64511e99a9a84ee5a5a4185cbf3d7041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.7821 |train_acc : 0.2625 |test_loss : 1.2120 |test_acc : 0.3438 \n",
      "Epoch 2 |train_loss : 1.0481 |train_acc : 0.4000 |test_loss : 1.0425 |test_acc : 0.4750 \n",
      "Epoch 3 |train_loss : 1.2253 |train_acc : 0.3187 |test_loss : 1.0436 |test_acc : 0.4545 \n",
      "Epoch 4 |train_loss : 1.2577 |train_acc : 0.2125 |test_loss : 1.0442 |test_acc : 0.4909 \n",
      "Epoch 5 |train_loss : 1.1849 |train_acc : 0.2333 |test_loss : 1.0304 |test_acc : 0.5250 \n",
      "Epoch 6 |train_loss : 1.1810 |train_acc : 0.2708 |test_loss : 1.0149 |test_acc : 0.5801 \n",
      "Epoch 7 |train_loss : 1.1857 |train_acc : 0.2729 |test_loss : 1.0058 |test_acc : 0.5898 \n",
      "Epoch 8 |train_loss : 1.1844 |train_acc : 0.2583 |test_loss : 0.9956 |test_acc : 0.6085 \n",
      "Epoch 9 |train_loss : 1.1851 |train_acc : 0.2458 |test_loss : 0.9901 |test_acc : 0.5960 \n",
      "Epoch 10 |train_loss : 1.1637 |train_acc : 0.2792 |test_loss : 0.9832 |test_acc : 0.6051 \n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 12\n",
      "[INFO] model: effnetb0\n",
      "[INFO] Optimizer: SGD\n",
      "[INFO] Epochs: 10\n",
      "[INFO] create new effntb2 model.\n",
      "[INFO] create summary writer, saving to: runs/2024-08-23/SGD/effnetb0/10\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb9e3d006da4ec5a5b9c61a9f7450eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 |train_loss : 1.1177 |train_acc : 0.2938 |test_loss : 1.0983 |test_acc : 0.3898 \n",
      "Epoch 2 |train_loss : 1.1257 |train_acc : 0.3000 |test_loss : 1.0995 |test_acc : 0.3489 \n",
      "Epoch 3 |train_loss : 1.1109 |train_acc : 0.3250 |test_loss : 1.0966 |test_acc : 0.3483 \n",
      "Epoch 4 |train_loss : 1.1160 |train_acc : 0.3375 |test_loss : 1.0948 |test_acc : 0.3483 \n",
      "Epoch 5 |train_loss : 1.1066 |train_acc : 0.3521 |test_loss : 1.0937 |test_acc : 0.3659 \n",
      "Epoch 6 |train_loss : 1.1101 |train_acc : 0.3500 |test_loss : 1.0914 |test_acc : 0.3875 \n",
      "Epoch 7 |train_loss : 1.1101 |train_acc : 0.3667 |test_loss : 1.0914 |test_acc : 0.3688 \n",
      "Epoch 8 |train_loss : 1.1038 |train_acc : 0.3771 |test_loss : 1.0886 |test_acc : 0.4051 \n",
      "Epoch 9 |train_loss : 1.1181 |train_acc : 0.3458 |test_loss : 1.0868 |test_acc : 0.4023 \n",
      "Epoch 10 |train_loss : 1.0967 |train_acc : 0.3646 |test_loss : 1.0864 |test_acc : 0.4051 \n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "\n",
    "experiment_number = 0\n",
    "model_num = 0\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for epochs in num_epochs:\n",
    "    for model in models_names:\n",
    "        for optimizer in optimizers:\n",
    "            experiment_number += 1\n",
    "            name = models_names[model_num]\n",
    "            print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "            print(f\"[INFO] model: {models_names[model_num]}\")\n",
    "            print(f\"[INFO] Optimizer: {optimizer}\")\n",
    "            print(f\"[INFO] Epochs: {epochs}\")\n",
    "            if model == \"effnetb0\":\n",
    "                model = create_effntb0()\n",
    "            else:\n",
    "                model = create_effntb2()\n",
    "            writer = create_write(name=optimizer, \n",
    "            model=models_names[model_num], extra=str(epochs))\n",
    "            log_dir = os.path.join(\"log\", timestamp + optimizer + name + str(epochs))\n",
    "            optimizer = select_optimizer(model, optimizer)\n",
    "            train(model, test_dataloader, train_dataloader, loss_fn, optimizer, device,\n",
    "            epochs, writer, log_dir)\n",
    "            print(\"-\" * 50 + \"\\n\")\n",
    "    model_num = ~model_num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
