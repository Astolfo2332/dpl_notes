{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "Code example [link](https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/fine-tuning/ft-example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03846842be454548950a162e7ece9db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6849417308924858956e3813ac006779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba0dc56018c403ba68027db86b2f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/853k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bce3a0305a4e9385ebf99f2af231af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb2a9f3f3a474698b1895cf5a8dfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Look up](https://huggingface.co/distilbert/distilbert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "#Label identification\n",
    "\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, id2label=id2label, label2id=label2id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    #Se debe adicionar el token de pad\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"}) \n",
    "    #TambiÃ©n se debe actualizar en el modelo los nuevos largos del token\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber how the training dataset is display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Disgused as an Asian Horror, \"A Tale Of Two Sisters\" is actually a complex character driven psychological drama, that engulfs the viewer into the problems of a seemingly normal family. I was really surprised at the depth of this movie. Director Ji-woon Kim\\'s decision to focus more on telling a story rather than providing cheap scares, has proved a correct one. Creating one of the most ingenious new horror movies.<br /><br />\"A Tale Of Two Sisters\" tels the story, as it\\'s name suggest of two sisters Su-mi and the younger Su-yeon, who after spending time in a mental institution return home to their father and apparently abusive stepmother. From then on we witness how the sisters deal with their stepmother\\'s gradually rising aggression and erratic behavior. To say what would happen next would be to be spoil the entire experience. So I\\'ll just leave it at that.<br /><br />The plot is very tightly written. With the characters nicely fleshed out. Ji-woon Kim\\'s focus on a small cast offers a much more detailed view on them and their relations to one another. Furthermore each of the four main cast has a vastly different role and type of character. From the protective Su-mi, the weaker Su-yeon, the visibly uninterested father to the stepmother\\'s frantic and later deadly behavior. There is great sense of mystery, with a lot of the plot not revealed up into the end and even after that the movie still leaves a great room for interpretation. Even after watching it once, the viewer will be compelled to see it at least once more so that he can gain a better understanding to it.<br /><br />The actors superbly fit their roles. It is especially hard to create strong, emotional scenes in psychological movies but it is a great joy when one succeeds in creating them and this is a prime example of such a feat. Ji-woon Kim\\'s direction is slow paced and gripping, building up tension for the film\\'s horroresque scenes. While few in number those moments are strong and quite frankly terrifying. The cinematography and score are top notch further helping to establish an atmosphere fitting that of a psychological film.<br /><br />\"A Tale Of Two Sisters\" is a demonstration how the horror genre is in fact so much more than a simple thrill ride. With it\\'s strong focus on character and mystery this is one complex movie that could easily seduce you in watching it again and again just so that you can understand it better.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_func(example: dict):\n",
    "    text = example[\"text\"]\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text, #We pass the text of the dataset\n",
    "        return_tensors=\"pt\", #The datatype that we want the output data\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512 #This is intrinsic to the len of the model\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7d79e9d4f24518bc34e05fcaa8930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326ab3d397c1476e8f2ac9843e669634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 21007,\n",
       " 2015,\n",
       " 1996,\n",
       " 13972,\n",
       " 2046,\n",
       " 1996,\n",
       " 3471,\n",
       " 1997,\n",
       " 1037,\n",
       " 9428,\n",
       " 3671,\n",
       " 2155,\n",
       " 1012,\n",
       " 1045,\n",
       " 2001,\n",
       " 2428,\n",
       " 4527,\n",
       " 2012,\n",
       " 1996,\n",
       " 5995,\n",
       " 1997,\n",
       " 2023,\n",
       " 3185,\n",
       " 1012,\n",
       " 2472,\n",
       " 10147,\n",
       " 1011,\n",
       " 15854,\n",
       " 2078,\n",
       " 5035,\n",
       " 1005,\n",
       " 1055,\n",
       " 3247,\n",
       " 2000,\n",
       " 3579,\n",
       " 2062,\n",
       " 2006,\n",
       " 4129,\n",
       " 1037,\n",
       " 2466,\n",
       " 2738,\n",
       " 2084,\n",
       " 4346,\n",
       " 10036,\n",
       " 29421,\n",
       " 1010,\n",
       " 2038,\n",
       " 4928,\n",
       " 1037,\n",
       " 6149,\n",
       " 2028,\n",
       " 1012,\n",
       " 4526,\n",
       " 2028,\n",
       " 1997,\n",
       " 1996,\n",
       " 2087,\n",
       " 13749,\n",
       " 18595,\n",
       " 3560,\n",
       " 2047,\n",
       " 5469,\n",
       " 5691,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1000,\n",
       " 1037,\n",
       " 6925,\n",
       " 1997,\n",
       " 2048,\n",
       " 5208,\n",
       " 1000,\n",
       " 10093,\n",
       " 2015,\n",
       " 1996,\n",
       " 2466,\n",
       " 1010,\n",
       " 2004,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2171,\n",
       " 6592,\n",
       " 1997,\n",
       " 2048,\n",
       " 5208,\n",
       " 10514,\n",
       " 1011,\n",
       " 2771,\n",
       " 1998,\n",
       " 1996,\n",
       " 3920,\n",
       " 10514,\n",
       " 1011,\n",
       " 6300,\n",
       " 2239,\n",
       " 1010,\n",
       " 2040,\n",
       " 2044,\n",
       " 5938,\n",
       " 2051,\n",
       " 1999,\n",
       " 1037,\n",
       " 5177,\n",
       " 5145,\n",
       " 2709,\n",
       " 2188,\n",
       " 2000,\n",
       " 2037,\n",
       " 2269,\n",
       " 1998,\n",
       " 4593,\n",
       " 20676,\n",
       " 26959,\n",
       " 1012,\n",
       " 2013,\n",
       " 2059,\n",
       " 2006,\n",
       " 2057,\n",
       " 7409,\n",
       " 2129,\n",
       " 1996,\n",
       " 5208,\n",
       " 3066,\n",
       " 2007,\n",
       " 2037,\n",
       " 26959,\n",
       " 1005,\n",
       " 1055,\n",
       " 6360,\n",
       " 4803,\n",
       " 14974,\n",
       " 1998,\n",
       " 24122,\n",
       " 5248,\n",
       " 1012,\n",
       " 2000,\n",
       " 2360,\n",
       " 2054,\n",
       " 2052,\n",
       " 4148,\n",
       " 2279,\n",
       " 2052,\n",
       " 2022,\n",
       " 2000,\n",
       " 2022,\n",
       " 27594,\n",
       " 1996,\n",
       " 2972,\n",
       " 3325,\n",
       " 1012,\n",
       " 2061,\n",
       " 1045,\n",
       " 1005,\n",
       " 2222,\n",
       " 2074,\n",
       " 2681,\n",
       " 2009,\n",
       " 2012,\n",
       " 2008,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 5436,\n",
       " 2003,\n",
       " 2200,\n",
       " 7371,\n",
       " 2517,\n",
       " 1012,\n",
       " 2007,\n",
       " 1996,\n",
       " 3494,\n",
       " 19957,\n",
       " 5771,\n",
       " 2098,\n",
       " 2041,\n",
       " 1012,\n",
       " 10147,\n",
       " 1011,\n",
       " 15854,\n",
       " 2078,\n",
       " 5035,\n",
       " 1005,\n",
       " 1055,\n",
       " 3579,\n",
       " 2006,\n",
       " 1037,\n",
       " 2235,\n",
       " 3459,\n",
       " 4107,\n",
       " 1037,\n",
       " 2172,\n",
       " 2062,\n",
       " 6851,\n",
       " 3193,\n",
       " 2006,\n",
       " 2068,\n",
       " 1998,\n",
       " 2037,\n",
       " 4262,\n",
       " 2000,\n",
       " 2028,\n",
       " 2178,\n",
       " 1012,\n",
       " 7297,\n",
       " 2169,\n",
       " 1997,\n",
       " 1996,\n",
       " 2176,\n",
       " 2364,\n",
       " 3459,\n",
       " 2038,\n",
       " 1037,\n",
       " 24821,\n",
       " 2367,\n",
       " 2535,\n",
       " 1998,\n",
       " 2828,\n",
       " 1997,\n",
       " 2839,\n",
       " 1012,\n",
       " 2013,\n",
       " 1996,\n",
       " 9474,\n",
       " 10514,\n",
       " 1011,\n",
       " 2771,\n",
       " 1010,\n",
       " 1996,\n",
       " 15863,\n",
       " 10514,\n",
       " 1011,\n",
       " 6300,\n",
       " 2239,\n",
       " 1010,\n",
       " 1996,\n",
       " 19397,\n",
       " 4895,\n",
       " 18447,\n",
       " 18702,\n",
       " 3064,\n",
       " 2269,\n",
       " 2000,\n",
       " 1996,\n",
       " 26959,\n",
       " 1005,\n",
       " 1055,\n",
       " 15762,\n",
       " 1998,\n",
       " 2101,\n",
       " 9252,\n",
       " 5248,\n",
       " 1012,\n",
       " 2045,\n",
       " 2003,\n",
       " 2307,\n",
       " 3168,\n",
       " 1997,\n",
       " 6547,\n",
       " 1010,\n",
       " 2007,\n",
       " 1037,\n",
       " 2843,\n",
       " 1997,\n",
       " 1996,\n",
       " 5436,\n",
       " 2025,\n",
       " 3936,\n",
       " 2039,\n",
       " 2046,\n",
       " 1996,\n",
       " 2203,\n",
       " 1998,\n",
       " 2130,\n",
       " 2044,\n",
       " 2008,\n",
       " 1996,\n",
       " 3185,\n",
       " 2145,\n",
       " 3727,\n",
       " 1037,\n",
       " 2307,\n",
       " 2282,\n",
       " 2005,\n",
       " 7613,\n",
       " 1012,\n",
       " 2130,\n",
       " 2044,\n",
       " 3666,\n",
       " 2009,\n",
       " 2320,\n",
       " 1010,\n",
       " 1996,\n",
       " 13972,\n",
       " 2097,\n",
       " 2022,\n",
       " 15055,\n",
       " 2000,\n",
       " 2156,\n",
       " 2009,\n",
       " 2012,\n",
       " 2560,\n",
       " 2320,\n",
       " 2062,\n",
       " 2061,\n",
       " 2008,\n",
       " 2002,\n",
       " 2064,\n",
       " 5114,\n",
       " 1037,\n",
       " 2488,\n",
       " 4824,\n",
       " 2000,\n",
       " 2009,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 5889,\n",
       " 21688,\n",
       " 2135,\n",
       " 4906,\n",
       " 2037,\n",
       " 4395,\n",
       " 1012,\n",
       " 2009,\n",
       " 2003,\n",
       " 2926,\n",
       " 2524,\n",
       " 2000,\n",
       " 3443,\n",
       " 2844,\n",
       " 1010,\n",
       " 6832,\n",
       " 5019,\n",
       " 1999,\n",
       " 8317,\n",
       " 5691,\n",
       " 2021,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 2307,\n",
       " 6569,\n",
       " 2043,\n",
       " 2028,\n",
       " 21645,\n",
       " 1999,\n",
       " 4526,\n",
       " 2068,\n",
       " 1998,\n",
       " 2023,\n",
       " 2003,\n",
       " 1037,\n",
       " 3539,\n",
       " 2742,\n",
       " 1997,\n",
       " 2107,\n",
       " 1037,\n",
       " 8658,\n",
       " 1012,\n",
       " 10147,\n",
       " 1011,\n",
       " 15854,\n",
       " 2078,\n",
       " 5035,\n",
       " 1005,\n",
       " 1055,\n",
       " 3257,\n",
       " 2003,\n",
       " 4030,\n",
       " 13823,\n",
       " 1998,\n",
       " 13940,\n",
       " 1010,\n",
       " 2311,\n",
       " 2039,\n",
       " 6980,\n",
       " 2005,\n",
       " 1996,\n",
       " 2143,\n",
       " 1005,\n",
       " 1055,\n",
       " 5469,\n",
       " 2229,\n",
       " 4226,\n",
       " 5019,\n",
       " 1012,\n",
       " 2096,\n",
       " 2261,\n",
       " 1999,\n",
       " 2193,\n",
       " 2216,\n",
       " 5312,\n",
       " 2024,\n",
       " 2844,\n",
       " 1998,\n",
       " 3243,\n",
       " 19597,\n",
       " 17082,\n",
       " 1012,\n",
       " 1996,\n",
       " 16434,\n",
       " 1998,\n",
       " 3556,\n",
       " 2024,\n",
       " 2327,\n",
       " 18624,\n",
       " 2582,\n",
       " 5094,\n",
       " 2000,\n",
       " 5323,\n",
       " 2019,\n",
       " 7224,\n",
       " 11414,\n",
       " 2008,\n",
       " 1997,\n",
       " 1037,\n",
       " 8317,\n",
       " 2143,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1000,\n",
       " 1037,\n",
       " 6925,\n",
       " 1997,\n",
       " 2048,\n",
       " 5208,\n",
       " 1000,\n",
       " 2003,\n",
       " 1037,\n",
       " 10467,\n",
       " 2129,\n",
       " 1996,\n",
       " 5469,\n",
       " 6907,\n",
       " 2003,\n",
       " 1999,\n",
       " 2755,\n",
       " 2061,\n",
       " 2172,\n",
       " 2062,\n",
       " 2084,\n",
       " 1037,\n",
       " 3722,\n",
       " 16959,\n",
       " 4536,\n",
       " 1012,\n",
       " 2007,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2844,\n",
       " 3579,\n",
       " 2006,\n",
       " 2839,\n",
       " 1998,\n",
       " 6547,\n",
       " 2023,\n",
       " 2003,\n",
       " 2028,\n",
       " 3375,\n",
       " 3185,\n",
       " 2008,\n",
       " 2071,\n",
       " 4089,\n",
       " 23199,\n",
       " 2017,\n",
       " 1999,\n",
       " 3666,\n",
       " 2009,\n",
       " 2153,\n",
       " 1998,\n",
       " 2153,\n",
       " 2074,\n",
       " 2061,\n",
       " 2008,\n",
       " 2017,\n",
       " 2064,\n",
       " 3305,\n",
       " 2009,\n",
       " 2488,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenized_dataset[\"validation\"][\"input_ids\"][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we have our dataset tokenized and with the atention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collactor to help us whit the dynamic padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29b17ba8f504ad381fa63ed19946833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can add another metrics if we want like F1\n",
    "def compute_metrics(p: tuple) -> dict:\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    #return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n",
    "    #Quick correction for new versions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horrible pelicula-Positive\n",
      "Excelente me gusto mucho-Positive\n",
      "This was a error-Positive\n",
      "This is a hard pass-Positive\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"Horrible pelicula\", \"Excelente me gusto mucho\",\n",
    "              \"This was a error\", \"This is a hard pass\"]\n",
    "for i in text_list:\n",
    "    inputs = tokenizer.encode(i, return_tensors=\"pt\").to(device)\n",
    "    logits = model(inputs).logits\n",
    "    prediction = torch.argmax(logits)\n",
    "    print(i + \"-\" + id2label[prediction.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", #The task that we want te model to perform\n",
    "                        r=4, \n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['q_lin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'q_lin'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the model to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config).to(device)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"../models/\" + model_name + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3b52c583564d718682aa5512f7ac7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddce9c246a14840af0ad747d2df9be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3868778944015503, 'eval_accuracy': 0.87, 'eval_runtime': 2.4688, 'eval_samples_per_second': 405.048, 'eval_steps_per_second': 101.262, 'epoch': 1.0}\n",
      "{'loss': 0.1898, 'grad_norm': 1.4144700765609741, 'learning_rate': 0.0008, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880711da9db7491fb82dffdbf96a5d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3392442464828491, 'eval_accuracy': 0.864, 'eval_runtime': 2.461, 'eval_samples_per_second': 406.332, 'eval_steps_per_second': 101.583, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bfda0d37224cfd89428fd108bd02e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3182048797607422, 'eval_accuracy': 0.866, 'eval_runtime': 2.4653, 'eval_samples_per_second': 405.636, 'eval_steps_per_second': 101.409, 'epoch': 3.0}\n",
      "{'loss': 0.192, 'grad_norm': 3.7050577361696924e-07, 'learning_rate': 0.0006, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d445c8e75ec944ce945aab4a52fc75ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4299923181533813, 'eval_accuracy': 0.866, 'eval_runtime': 2.4782, 'eval_samples_per_second': 403.514, 'eval_steps_per_second': 100.878, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96db324723e46dfbaf4405fadd17127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6296740770339966, 'eval_accuracy': 0.871, 'eval_runtime': 2.5042, 'eval_samples_per_second': 399.337, 'eval_steps_per_second': 99.834, 'epoch': 5.0}\n",
      "{'loss': 0.109, 'grad_norm': 0.0002020986139541492, 'learning_rate': 0.0004, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1b3bcac1a4469d908f31d75e559b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6260613203048706, 'eval_accuracy': 0.873, 'eval_runtime': 2.4915, 'eval_samples_per_second': 401.366, 'eval_steps_per_second': 100.341, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d380c06f6a3c4ff5918321ad5e8bea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8962013721466064, 'eval_accuracy': 0.867, 'eval_runtime': 2.4873, 'eval_samples_per_second': 402.05, 'eval_steps_per_second': 100.513, 'epoch': 7.0}\n",
      "{'loss': 0.033, 'grad_norm': 0.0015833841171115637, 'learning_rate': 0.0002, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fe432afade4ede8e6e72e5c87295b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8780865669250488, 'eval_accuracy': 0.867, 'eval_runtime': 2.4873, 'eval_samples_per_second': 402.05, 'eval_steps_per_second': 100.513, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1463df46295e45dbbb400076f3c21ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8227405548095703, 'eval_accuracy': 0.868, 'eval_runtime': 2.4787, 'eval_samples_per_second': 403.44, 'eval_steps_per_second': 100.86, 'epoch': 9.0}\n",
      "{'loss': 0.0212, 'grad_norm': 0.05187961831688881, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9156c17769964267a26788c174e3235a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7314329147338867, 'eval_accuracy': 0.872, 'eval_runtime': 2.5015, 'eval_samples_per_second': 399.759, 'eval_steps_per_second': 99.94, 'epoch': 10.0}\n",
      "{'train_runtime': 82.4113, 'train_samples_per_second': 121.343, 'train_steps_per_second': 30.336, 'train_loss': 0.10900678215026856, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.10900678215026856, metrics={'train_runtime': 82.4113, 'train_samples_per_second': 121.343, 'train_steps_per_second': 30.336, 'total_flos': 1343996682240000.0, 'train_loss': 0.10900678215026856, 'epoch': 10.0})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horrible pelicula-Negative\n",
      "Excelente me gusto mucho-Positive\n",
      "Nunca vi algo tan horrible-Negative\n",
      "De las mejores peliculas que e visto-Positive\n",
      "This was a error-Negative\n",
      "This is a hard pass-Positive\n",
      "It was good.-Positive\n",
      "Not a fan, don't recommed.-Negative\n",
      "Better than the first one.-Positive\n",
      "This is not worth watching even once.-Negative\n",
      "This one is a pass.-Positive\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"Horrible pelicula\", \"Excelente me gusto mucho\", \"Nunca vi algo tan horrible\",\n",
    "             \"De las mejores peliculas que e visto\",\n",
    "              \"This was a error\", \"This is a hard pass\"]+[\"It was good.\",\n",
    "             \"Not a fan, don't recommed.\", \"Better than the first one.\",\n",
    "              \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "for i in text_list:\n",
    "    inputs = tokenizer.encode(i, return_tensors=\"pt\").to(device)\n",
    "    logits = model(inputs).logits\n",
    "    prediction = torch.argmax(logits)\n",
    "    print(i + \"-\" + id2label[prediction.tolist()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
